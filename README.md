## Explaining file attached to the repo
1. **Web_Scrapper (App)** - In the folder we have added the Python code and app code we created using Streamlit, there are two main files one is the class we created with the name 'TrustPilotScraper', and then we created the code in it to scrapped the desired organization from "trust-pilot" and several pages you want to scrap. The other file App.py is the important file to initiate the app in the streamlit local host. 

2. **Data_Scrapped** - This is the CSV file in which the data collected from the app has been stored for further analysis. This file contains data from two companies "One Call Insurance" and "ROC". The CSV File contains multiple columns which the app created an extract from the "Trust_pilot"

3. **Project-After_Web_Scrapping**- This file contains the data wrangling and analysis of all data_scrapped throughout the phase and follows the path as described in the document to guide the project.


#### General Observations:- If you try to send more than 200 requests in 2 mins then "Trust-Pilot" might block your IP Address. 


This project is made towards the final submission at ESILV, for the subject - Machine Learning for NLP, 
The project is a group effort, made possible by the teamwork of Shubham SAINI & Manisha RAWALA.

## FOR THE DETAIL UNDERSTANDING OF EACH ASPECT OF THE PROJECT WE INVITE YOU TO FOLLOW THE DETAILED REPORT ON THE PROJECT:
 link - https://docs.google.com/document/d/1T5NR2hwJ8TvlK9i_WUqfrviNwHRO_HMy7Q1uliYXRg0/edit?usp=sharing
